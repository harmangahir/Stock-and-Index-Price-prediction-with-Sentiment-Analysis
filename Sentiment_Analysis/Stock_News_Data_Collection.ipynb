{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "import nltk\n",
    "# Load, explore, process and plot data\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle as pk\n",
    "\n",
    "#Text Preprocessing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_blog_url(soup):\n",
    "    div_ = soup.find_all('div', attrs = {'class':'FL PR20'})\n",
    "    url_list = []\n",
    "    for title in div_:\n",
    "        href = title.find('a')['href']\n",
    "        url_list.append(\"https://www.moneycontrol.com/\"+href)\n",
    "    return url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_blog_content(url):\n",
    "    request = requests.get(url)\n",
    "    print('Received URL',url)\n",
    "    soup = bs4.BeautifulSoup(request.text,'html.parser')\n",
    "    all_script = soup.find_all('script',attrs = {'type':'application/ld+json'})\n",
    "    raw_article_str = all_script[2].get_text().replace('\\r\\n',' ')\n",
    "    parts = re.split(r\"\"\"(\"[^\"]*\"|'[^']*')\"\"\", raw_article_str)\n",
    "    parts[::2] = map(lambda s: \"\".join(s.split()), parts[::2])\n",
    "    article_str = \"\".join(parts)\n",
    "    article_str = article_str[1:]\n",
    "    article_str = article_str[:-1]\n",
    "    article_dict = json.loads(article_str, strict=False)\n",
    "    all_tags = soup.find_all('div',attrs={'class':'tags_first_line'})\n",
    "    list_all_tags=[]\n",
    "    for i in all_tags:\n",
    "        list_all_tags.append(i.get_text())\n",
    "    tags = list_all_tags[0].replace('Tags','')\n",
    "    tags = tags.replace('\\n','')\n",
    "    tags = tags.split('#')\n",
    "    tags = tags[1:]\n",
    "    tags = ', '.join([str(elem).strip() for elem in tags])\n",
    "    article_dict['tags'] = tags\n",
    "    return article_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_no(url, sc_id, page_no, next, year):\n",
    "    request = requests.get(url)\n",
    "    soup = BeautifulSoup(request.text,'html.parser')\n",
    "\n",
    "    \n",
    "    all_page_no = soup.find_all('div', attrs = {'class': 'pages MR10 MT15'})\n",
    "    page_list  = [i.text for i in all_page_no[0].find_all('a')]\n",
    "\n",
    "\n",
    "    if any(map(str.isdigit,page_list[-1])):\n",
    "        return int(page_list[-1]), next\n",
    "    else:\n",
    "        next = next + 1\n",
    "        page_no = int(page_list[-2])\n",
    "        url = \"https://www.moneycontrol.com/stocks/company_info/stock_news.php?sc_id=\"+sc_id+\"&scat=&pageno=\"+str(page_no)+\"&next=\"+str(next)+\"&durationType=Y&Year=\"+str(year)+\"&duration=1&news_type=\"\n",
    "    return get_page_no(url, sc_id, page_no, next, year)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_company_data(url_ = \"https://www.moneycontrol.com/stocks/company_info/stock_news.php?\", sc_id=[], pageno=1, next=0, years=[]):\n",
    "    for company in sc_id:\n",
    "        df = pd.DataFrame(columns = ['company', 'datePublished', 'author', 'headline',\n",
    "                                     'description', 'articleBosy','tags','url'])\n",
    "        for year in years:\n",
    "            print('Year: ', year)\n",
    "            print('Page_No:', pageno)\n",
    "            print('Next: ', next)\n",
    "\n",
    "            url = url_ + \"sc_id=\"+company+\"&scat=&pageno=\"+str(pageno)+\"&next=\"+str(next)+\"&durationType=Y&Year=\"+str(year)+\"&duration=1&news_type=\"\n",
    "            print('url: ', url)\n",
    "\n",
    "            max_page_no, max_next = get_page_no(url, company, pageno, next, year)\n",
    "            max_next = max_next + 1\n",
    "            print('Total Page:',max_page_no)\n",
    "            for i in range(max_next):\n",
    "                for j in range((i*10)+1, (i*10)+11):\n",
    "                    if j <= max_page_no:\n",
    "                        url_list = []\n",
    "                        url = url_ + \"sc_id=\"+company+\"&scat=&pageno=\"+str(j)+\"&next=\"+str(i)+\"&durationType=Y&Year=\"+str(year)+\"&duration=1&news_type=\"\n",
    "                        print(url)\n",
    "                        request = requests.get(url)\n",
    "                        soup = BeautifulSoup(request.text, 'html.parser')\n",
    "                        \n",
    "                        url_list = get_blog_url(soup)\n",
    "                        \n",
    "                        frame1 = []\n",
    "\n",
    "                        \n",
    "                        for url in url_list:\n",
    "                            try:\n",
    "                                #print('Blog URL:',url)\n",
    "                                article_dict = get_blog_content(url)\n",
    "\n",
    "                                \n",
    "                                print(company)\n",
    "                                print(article_dict['datePublished'])\n",
    "                                print(article_dict['author'])\n",
    "                                print(article_dict['headline'])\n",
    "                                print(article_dict['description'])\n",
    "                                print(article_dict['articleBody'])\n",
    "                                print(article_dict['tags'])\n",
    "                                print(article_dict['url'])\n",
    "                                print('--------------------------------------------------------')\n",
    "\n",
    "                                article_lst = [[company,\n",
    "                                                 article_dict['datePublished'],\n",
    "                                                 article_dict['author'],\n",
    "                                                 article_dict['headline'],\n",
    "                                                 article_dict['description'],\n",
    "                                                 article_dict['articleBody'],\n",
    "                                                 article_dict['tags'],\n",
    "                                                 url]]\n",
    "                               \n",
    "       \n",
    "                                df = pd.concat([df,pd.DataFrame(article_lst, columns=['company','datePublished','author','headline',\n",
    "                                                                                   'description','articleBody','tags','url'])],axis=0, ignore_index = True)\n",
    "                               \n",
    "\n",
    "                            except:\n",
    "                                \n",
    "                                article_lst = [[company, 'error','error','error','error','error','error',url]]\n",
    "                                \n",
    "     \n",
    "                                df = pd.concat([df,pd.DataFrame(article_lst, columns=['company','datePublished','author','headline',\n",
    "                                                                                   'description','articleBody','tags','url'])],axis=0, ignore_index = True)\n",
    "                               \n",
    "                                continue\n",
    "                           \n",
    "        df.to_csv(company+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_company_data(sc_id=['RI'], years = [2008,2007])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('News_Data/RI(2011_2023).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['S. No.', 'company', 'datePublished', 'author', 'headline',\n",
       "       'description', 'tags', 'url', 'articleBody'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns\n",
    "df = df.drop(['S. No.'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_sentiments = pd.DataFrame({\n",
    "                        'neg':[],\n",
    "                        'neu':[],\n",
    "                        'pos':[],\n",
    "                        'compound':[]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(sentence):\n",
    "  sentence = re.sub(r\"n\\'t\", \" not\", sentence)\n",
    "  sentence = re.sub(r\"\\'re\", \" are\", sentence)\n",
    "  sentence = re.sub(r\"\\'s\", \" is\", sentence)\n",
    "  sentence = re.sub(r\"\\'d\", \" would\", sentence)\n",
    "  sentence = re.sub(r\"\\'ll\", \" will\", sentence)\n",
    "  sentence = re.sub(r\"\\'t\", \" not\", sentence)\n",
    "  sentence = re.sub(r\"\\'ve\", \" have\", sentence)\n",
    "  sentence = re.sub(r\"\\'m\", \" am\", sentence)\n",
    "  sentence = re.sub(r\"wont\", \"will not\", sentence)\n",
    "  sentence = re.sub(r\"wouldnt\", \"would not\", sentence)\n",
    "  sentence = re.sub(r\"shouldnt\", \"should not\", sentence)\n",
    "  sentence = re.sub(r\"couldnt\", \"could not\", sentence)\n",
    "  sentence = re.sub(r\"cudnt\", \"could not\", sentence)\n",
    "  sentence = re.sub(r\"cant\", \"can not\", sentence)\n",
    "  sentence = re.sub(r\"dont\", \"do not\", sentence)\n",
    "  sentence = re.sub(r\"doesnt\", \"does not\", sentence)\n",
    "  sentence = re.sub(r\"didnt\", \"did not\", sentence)\n",
    "  sentence = re.sub(r\"wasnt\", \"was not\", sentence)\n",
    "  sentence = re.sub(r\"werent\", \"were not\", sentence)\n",
    "  sentence = re.sub(r\"havent\", \"have not\", sentence)\n",
    "  sentence = re.sub(r\"hadnt\", \"had not\", sentence)\n",
    "  sentence = re.sub(r\"ain't\", \"am not\",sentence)\n",
    "  sentence = re.sub(r\"aren't\", \"are not\",sentence)\n",
    "  sentence = re.sub(r\"can't\", \"cannot\",sentence)\n",
    "  sentence = re.sub(r\"can't've\", \"cannot have\",sentence)\n",
    "  sentence = re.sub(r\"'cause\", \"because\",sentence)\n",
    "  sentence = re.sub(r\"could've\", \"could have\",sentence)\n",
    "  sentence = re.sub(r\"couldn't\", \"could not\",sentence)\n",
    "  sentence = re.sub(r\"couldn't've\", \"could not have\",sentence)\n",
    "  sentence = re.sub(r\"didn't\", \"did not\",sentence)\n",
    "  sentence = re.sub(r\"doesn't\", \"does not\",sentence)\n",
    "  sentence = re.sub(r\"don't\", \"do not\",sentence)\n",
    "  sentence = re.sub(r\"hadn't\", \"had not\",sentence)\n",
    "  sentence = re.sub(r\"hadn't've\", \"had not have\",sentence)\n",
    "  sentence = re.sub(r\"hasn't\", \"has not\",sentence)\n",
    "  sentence = re.sub(r\"haven't\", \"have not\",sentence)\n",
    "  sentence = re.sub(r\"he'd\", \"he would\",sentence)\n",
    "  sentence = re.sub(r\"he'd've\", \"he would have\",sentence)\n",
    "  sentence = re.sub(r\"he'll\", \"he will\",sentence)\n",
    "  sentence = re.sub(r\"he'll've\", \"he will have\",sentence)\n",
    "  sentence = re.sub(r\"he's\", \"he is\",sentence)\n",
    "  sentence = re.sub(r\"how'd\", \"how did\",sentence)\n",
    "  sentence = re.sub(r\"how'd'y\", \"how do you\",sentence)\n",
    "  sentence = re.sub(r\"how'll\", \"how will\",sentence)\n",
    "  sentence = re.sub(r\"how's\", \"how is\",sentence)\n",
    "  sentence = re.sub(r\"i'd\", \"I would\",sentence)\n",
    "  sentence = re.sub(r\"i'd've\", \"I would have\",sentence)\n",
    "  sentence = re.sub(r\"i'll\", \"I will\",sentence)\n",
    "  sentence = re.sub(r\"i'll've\", \"I will have\",sentence)\n",
    "  sentence = re.sub(r\"i'm\", \"I am\",sentence)\n",
    "  sentence = re.sub(r\"i've\", \"I have\",sentence)\n",
    "  sentence = re.sub(r\"isn't\", \"is not\",sentence)\n",
    "  sentence = re.sub(r\"it'd\", \"it had\",sentence)\n",
    "  sentence = re.sub(r\"it'd've\", \"it would have\",sentence)\n",
    "  sentence = re.sub(r\"it'll\", \"it will\",sentence)\n",
    "  sentence = re.sub(r\"it'll've\", \"it will have\",sentence)\n",
    "  sentence = re.sub(r\"it's\", \"it is\",sentence)\n",
    "  sentence = re.sub(r\"let's\", \"let us\",sentence)\n",
    "  sentence = re.sub(r\"ma'am\", \"madam\",sentence)\n",
    "  sentence = re.sub(r\"mayn't\", \"may not\",sentence)\n",
    "  sentence = re.sub(r\"might've\", \"might have\",sentence)\n",
    "  sentence = re.sub(r\"mightn't\", \"might not\",sentence)\n",
    "  sentence = re.sub(r\"mightn't've\", \"might not have\",sentence)\n",
    "  sentence = re.sub(r\"must've\", \"must have\",sentence)\n",
    "  sentence = re.sub(r\"mustn't\", \"must not\",sentence)\n",
    "  sentence = re.sub(r\"mustn't've\", \"must not have\",sentence)\n",
    "  sentence = re.sub(r\"needn't\", \"need not\",sentence)\n",
    "  sentence = re.sub(r\"needn't've\", \"need not have\",sentence)\n",
    "  sentence = re.sub(r\"o'clock\", \"of the clock\",sentence)\n",
    "  sentence = re.sub(r\"oughtn't\", \"ought not\",sentence)\n",
    "  sentence = re.sub(r\"oughtn't've\", \"ought not have\",sentence)\n",
    "  sentence = re.sub(r\"shan't\", \"shall not\",sentence)\n",
    "  sentence = re.sub(r\"sha'n't\", \"shall not\",sentence)\n",
    "  sentence = re.sub(r\"shan't've\", \"shall not have\",sentence)\n",
    "  sentence = re.sub(r\"she'd\", \"she would\",sentence)\n",
    "  sentence = re.sub(r\"she'd've\", \"she would have\",sentence)\n",
    "  sentence = re.sub(r\"she'll\", \"she will\",sentence)\n",
    "  sentence = re.sub(r\"she'll've\", \"she will have\",sentence)\n",
    "  sentence = re.sub(r\"she's\", \"she is\",sentence)\n",
    "  sentence = re.sub(r\"should've\", \"should have\",sentence)\n",
    "  sentence = re.sub(r\"shouldn't\", \"should not\",sentence)\n",
    "  sentence = re.sub(r\"shouldn't've\", \"should not have\",sentence)\n",
    "  sentence = re.sub(r\"so've\", \"so have\",sentence)\n",
    "  sentence = re.sub(r\"so's\", \"so is\",sentence)\n",
    "  sentence = re.sub(r\"that'd\", \"that would\",sentence)\n",
    "  sentence = re.sub(r\"that'd've\", \"that would have\",sentence)\n",
    "  sentence = re.sub(r\"that's\", \"that is\",sentence)\n",
    "  sentence = re.sub(r\"there'd\", \"there had\",sentence)\n",
    "  sentence = re.sub(r\"there'd've\", \"there would have\",sentence)\n",
    "  sentence = re.sub(r\"there's\", \"there is\",sentence)\n",
    "  sentence = re.sub(r\"they'd\", \"they would\",sentence)\n",
    "  sentence = re.sub(r\"they'd've\", \"they would have\",sentence)\n",
    "  sentence = re.sub(r\"they'll\", \"they will\",sentence)\n",
    "  sentence = re.sub(r\"they'll've\", \"they will have\",sentence)\n",
    "  sentence = re.sub(r\"they're\", \"they are\",sentence)\n",
    "  sentence = re.sub(r\"they've\", \"they have\",sentence)\n",
    "  sentence = re.sub(r\"to've\", \"to have\",sentence)\n",
    "  sentence = re.sub(r\"wasn't\", \"was not\",sentence)\n",
    "  sentence = re.sub(r\"we'd\", \"we had\",sentence)\n",
    "  sentence = re.sub(r\"we'd've\", \"we would have\",sentence)\n",
    "  sentence = re.sub(r\"we'll\", \"we will\",sentence)\n",
    "  sentence = re.sub(r\"we'll've\", \"we will have\",sentence)\n",
    "  sentence = re.sub(r\"we're\", \"we are\",sentence)\n",
    "  sentence = re.sub(r\"we've\", \"we have\",sentence)\n",
    "  sentence = re.sub(r\"weren't\", \"were not\",sentence)\n",
    "  sentence = re.sub(r\"what'll\", \"what will\",sentence)\n",
    "  sentence = re.sub(r\"what'll've\", \"what will have\",sentence)\n",
    "  sentence = re.sub(r\"what're\", \"what are\",sentence)\n",
    "  sentence = re.sub(r\"what's\", \"what is\",sentence)\n",
    "  sentence = re.sub(r\"what've\", \"what have\",sentence)\n",
    "  sentence = re.sub(r\"when's\", \"when is\",sentence)\n",
    "  sentence = re.sub(r\"when've\", \"when have\",sentence)\n",
    "  sentence = re.sub(r\"where'd\", \"where did\",sentence)\n",
    "  sentence = re.sub(r\"where's\", \"where is\",sentence)\n",
    "  sentence = re.sub(r\"where've\", \"where have\",sentence)\n",
    "  sentence = re.sub(r\"who'll\", \"who will\",sentence)\n",
    "  sentence = re.sub(r\"who'll've\", \"who will have\",sentence)\n",
    "  sentence = re.sub(r\"who's\", \"who is\",sentence)\n",
    "  sentence = re.sub(r\"who've\", \"who have\",sentence)\n",
    "  sentence = re.sub(r\"why's\", \"why is\",sentence)\n",
    "  sentence = re.sub(r\"why've\", \"why have\",sentence)\n",
    "  sentence = re.sub(r\"will've\", \"will have\",sentence)\n",
    "  sentence = re.sub(r\"won't\", \"will not\",sentence)\n",
    "  sentence = re.sub(r\"won't've\", \"will not have\",sentence)\n",
    "  sentence = re.sub(r\"would've\", \"would have\",sentence)\n",
    "  sentence = re.sub(r\"wouldn't\", \"would not\",sentence)\n",
    "  sentence = re.sub(r\"wouldn't've\", \"would not have\",sentence)\n",
    "  sentence = re.sub(r\"y'all\", \"you all\",sentence)\n",
    "  sentence = re.sub(r\"y'alls\", \"you alls\",sentence)\n",
    "  sentence = re.sub(r\"y'all'd\", \"you all would\",sentence)\n",
    "  sentence = re.sub(r\"y'all'd've\", \"you all would have\",sentence)\n",
    "  sentence = re.sub(r\"y'all're\", \"you all are\",sentence)\n",
    "  sentence = re.sub(r\"y'all've\", \"you all have\",sentence)\n",
    "  sentence = re.sub(r\"you'd\", \"you had\",sentence)\n",
    "  sentence = re.sub(r\"you'd've\", \"you would have\",sentence)\n",
    "  sentence = re.sub(r\"you'll\", \"you you will\",sentence)\n",
    "  sentence = re.sub(r\"you'll've\", \"you you will have\",sentence)\n",
    "  sentence = re.sub(r\"you're\", \"you are\",sentence)\n",
    "  sentence = re.sub(r\"you've\", \"you have\",sentence)\n",
    "\n",
    "  \n",
    "  sentence = re.sub(r'[^\\w\\s]','',sentence) # Remove Punctutation\n",
    "  \n",
    "  sentence = sentence.lower() # Lower case\n",
    "  \n",
    "  CLEANR = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "  sentence = re.sub(CLEANR, '', sentence)# Remove HTML\n",
    "  \n",
    "  sentence = re.sub('[^A-Za-z]',' ',sentence)# Remove Digits\n",
    " \n",
    "  sentence = re.sub(r'[\\w._%+-]{1,20}@[\\w.-]{1,20}.[A-Za-z]{2,3}','', sentence)# Remove Email\n",
    "  \n",
    "  sentence = re.sub(r'^http?s:\\/\\/.*[\\r\\n]*','', sentence)# Remove URLs\n",
    "  \n",
    "  sentence = re.sub(r'@[A-Za-z0-9]+','',sentence) #Remove Mentions  \n",
    "\n",
    "  sentence = \" \".join(item for item in sentence.split() if item not in stopwords.words('english') )\n",
    "  \n",
    "  sentence = ' '.join([PorterStemmer().stem(word)for word in sentence.split()])\n",
    "\n",
    "  sentence = ' '.join([WordNetLemmatizer().lemmatize(word) for word in sentence.split()])\n",
    "  return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "for text in df.articleBody:\n",
    "    preprocessedText = preprocessing(text)\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    polarity = sid.polarity_scores(str(preprocessedText))    \n",
    "    tmpdic = {}    \n",
    "    tmpdic.update(polarity)\n",
    "    article_sentiments= pd.concat([article_sentiments,pd.DataFrame([tmpdic])], ignore_index=True)    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_sentiments.to_csv(\"News_Data/article_sentiments_RI_2011-2023.csv\", sep=',', encoding='utf-8', header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
