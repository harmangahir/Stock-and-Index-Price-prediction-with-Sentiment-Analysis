{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "import nltk\n",
    "# Load, explore, process and plot data\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle as pk\n",
    "\n",
    "#Text Preprocessing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_blog_url(soup):\n",
    "    div_ = soup.find_all('div', attrs = {'class':'FL PR20'})\n",
    "    url_list = []\n",
    "    for title in div_:\n",
    "        href = title.find('a')['href']\n",
    "        url_list.append(\"https://www.moneycontrol.com/\"+href)\n",
    "    return url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_blog_content(url):\n",
    "    request = requests.get(url)\n",
    "    print('Received URL',url)\n",
    "    soup = bs4.BeautifulSoup(request.text,'html.parser')\n",
    "    all_script = soup.find_all('script',attrs = {'type':'application/ld+json'})\n",
    "    raw_article_str = all_script[2].get_text().replace('\\r\\n',' ')\n",
    "    parts = re.split(r\"\"\"(\"[^\"]*\"|'[^']*')\"\"\", raw_article_str)\n",
    "    parts[::2] = map(lambda s: \"\".join(s.split()), parts[::2])\n",
    "    article_str = \"\".join(parts)\n",
    "    article_str = article_str[1:]\n",
    "    article_str = article_str[:-1]\n",
    "    article_dict = json.loads(article_str, strict=False)\n",
    "    all_tags = soup.find_all('div',attrs={'class':'tags_first_line'})\n",
    "    list_all_tags=[]\n",
    "    for i in all_tags:\n",
    "        list_all_tags.append(i.get_text())\n",
    "    tags = list_all_tags[0].replace('Tags','')\n",
    "    tags = tags.replace('\\n','')\n",
    "    tags = tags.split('#')\n",
    "    tags = tags[1:]\n",
    "    tags = ', '.join([str(elem).strip() for elem in tags])\n",
    "    article_dict['tags'] = tags\n",
    "    return article_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_no(url, sc_id, page_no, next, year):\n",
    "    request = requests.get(url)\n",
    "    soup = BeautifulSoup(request.text,'html.parser')\n",
    "\n",
    "    \n",
    "    all_page_no = soup.find_all('div', attrs = {'class': 'pages MR10 MT15'})\n",
    "    page_list  = [i.text for i in all_page_no[0].find_all('a')]\n",
    "\n",
    "\n",
    "    if any(map(str.isdigit,page_list[-1])):\n",
    "        return int(page_list[-1]), next\n",
    "    else:\n",
    "        next = next + 1\n",
    "        page_no = int(page_list[-2])\n",
    "        url = \"https://www.moneycontrol.com/stocks/company_info/stock_news.php?sc_id=\"+sc_id+\"&scat=&pageno=\"+str(page_no)+\"&next=\"+str(next)+\"&durationType=Y&Year=\"+str(year)+\"&duration=1&news_type=\"\n",
    "    return get_page_no(url, sc_id, page_no, next, year)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_company_data(url_ = \"https://www.moneycontrol.com/stocks/company_info/stock_news.php?\", sc_id=[], pageno=1, next=0, years=[]):\n",
    "    for company in sc_id:\n",
    "        df = pd.DataFrame(columns = ['company', 'datePublished', 'author', 'headline',\n",
    "                                     'description', 'articleBosy','tags','url'])\n",
    "        for year in years:\n",
    "            print('Year: ', year)\n",
    "            print('Page_No:', pageno)\n",
    "            print('Next: ', next)\n",
    "\n",
    "            url = url_ + \"sc_id=\"+company+\"&scat=&pageno=\"+str(pageno)+\"&next=\"+str(next)+\"&durationType=Y&Year=\"+str(year)+\"&duration=1&news_type=\"\n",
    "            print('url: ', url)\n",
    "\n",
    "            max_page_no, max_next = get_page_no(url, company, pageno, next, year)\n",
    "            max_next = max_next + 1\n",
    "            print('Total Page:',max_page_no)\n",
    "            for i in range(max_next):\n",
    "                for j in range((i*10)+1, (i*10)+11):\n",
    "                    if j <= max_page_no:\n",
    "                        url_list = []\n",
    "                        url = url_ + \"sc_id=\"+company+\"&scat=&pageno=\"+str(j)+\"&next=\"+str(i)+\"&durationType=Y&Year=\"+str(year)+\"&duration=1&news_type=\"\n",
    "                        print(url)\n",
    "                        request = requests.get(url)\n",
    "                        soup = BeautifulSoup(request.text, 'html.parser')\n",
    "                        \n",
    "                        url_list = get_blog_url(soup)\n",
    "                        \n",
    "                        frame1 = []\n",
    "\n",
    "                        \n",
    "                        for url in url_list:\n",
    "                            try:\n",
    "                                #print('Blog URL:',url)\n",
    "                                article_dict = get_blog_content(url)\n",
    "\n",
    "                                \n",
    "                                print(company)\n",
    "                                print(article_dict['datePublished'])\n",
    "                                print(article_dict['author'])\n",
    "                                print(article_dict['headline'])\n",
    "                                print(article_dict['description'])\n",
    "                                print(article_dict['articleBody'])\n",
    "                                print(article_dict['tags'])\n",
    "                                print(article_dict['url'])\n",
    "                                print('--------------------------------------------------------')\n",
    "\n",
    "                                article_lst = [[company,\n",
    "                                                 article_dict['datePublished'],\n",
    "                                                 article_dict['author'],\n",
    "                                                 article_dict['headline'],\n",
    "                                                 article_dict['description'],\n",
    "                                                 article_dict['articleBody'],\n",
    "                                                 article_dict['tags'],\n",
    "                                                 url]]\n",
    "                               \n",
    "       \n",
    "                                df = pd.concat([df,pd.DataFrame(article_lst, columns=['company','datePublished','author','headline',\n",
    "                                                                                   'description','articleBody','tags','url'])],axis=0, ignore_index = True)\n",
    "                               \n",
    "\n",
    "                            except:\n",
    "                                \n",
    "                                article_lst = [[company, 'error','error','error','error','error','error',url]]\n",
    "                                \n",
    "     \n",
    "                                df = pd.concat([df,pd.DataFrame(article_lst, columns=['company','datePublished','author','headline',\n",
    "                                                                                   'description','articleBody','tags','url'])],axis=0, ignore_index = True)\n",
    "                               \n",
    "                                continue\n",
    "                           \n",
    "        df.to_csv(company+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_company_data(sc_id=['RI'], years = [2008,2007])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('News_Data/RI(2011_2023).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['S. No.', 'company', 'datePublished', 'author', 'headline',\n",
       "       'description', 'tags', 'url', 'articleBody'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns\n",
    "df = df.drop(['S. No.'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_sentiments = pd.DataFrame({\n",
    "                        'neg':[],\n",
    "                        'neu':[],\n",
    "                        'pos':[],\n",
    "                        'compound':[]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "for text in df.articleBody:\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    polarity = sid.polarity_scores(str(text))    \n",
    "    tmpdic = {}    \n",
    "    tmpdic.update(polarity)\n",
    "    article_sentiments= pd.concat([article_sentiments,pd.DataFrame([tmpdic])], ignore_index=True)    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_sentiments.to_csv(\"News_Data/article_sentiments_RI_2011-2023.csv\", sep=',', encoding='utf-8', header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
